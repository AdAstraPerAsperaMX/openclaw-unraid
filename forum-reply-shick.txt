Hey shick! Thanks for testing. Let me address each of your points:

**1. Telegram pairing**
After setting the bot token (either via the template's TELEGRAM_BOT_TOKEN variable or in the config), restart the container. Then DM your bot in Telegram. On first contact, it will generate a pairing code — you'll need to approve it in the Control UI under the pairing section, or via logs. Once approved, you're connected!

Quick config via Raw JSON if you want to skip pairing and allowlist yourself directly:

{
  "channels": {
    "telegram": {
      "enabled": true,
      "botToken": "YOUR_BOT_TOKEN",
      "dmPolicy": "allowlist",
      "allowFrom": ["tg:YOUR_TELEGRAM_USER_ID"]
    }
  }
}

Full Telegram docs: https://docs.openclaw.ai/channels/telegram

**2. CLI not working**
You're right — the `openclaw` command doesn't work out of the box in the container console. This is a Docker limitation we're aware of. For now, most configuration can be done via the Control UI. We're looking into options but don't have a fix yet.

**3. Local Ollama support**
Good news — OpenClaw does support Ollama! The catch for Docker: Ollama typically runs on your Unraid host, but the container needs to reach it. You'd need to configure the baseUrl to point to your host's IP.

In your config via Raw JSON:

{
  "agents": {
    "defaults": {
      "model": { "primary": "ollama/llama3.3" }
    }
  },
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://YOUR_UNRAID_IP:11434/v1",
        "apiKey": "ollama-local"
      }
    }
  }
}

Replace YOUR_UNRAID_IP with your Unraid server's IP (not localhost, since that would point inside the container). Make sure Ollama is running and accessible.

Full Ollama docs: https://docs.openclaw.ai/providers/ollama

**4. Primary/fallback models ("merge")**
Also supported! OpenClaw has built-in model fallback. If your primary model fails (rate limit, auth issue, etc.), it can automatically try fallback models.

{
  "agents": {
    "defaults": {
      "model": {
        "primary": "anthropic/claude-sonnet-4-5",
        "fallbacks": ["openai/gpt-4o", "google/gemini-2.0-flash"]
      }
    }
  }
}

This will try Claude first, then GPT-4o, then Gemini if the others fail.

Hope that helps! Let me know if you run into any issues.
